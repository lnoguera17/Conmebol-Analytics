---
title: "Modein Conmebol"
author: "Luis Noguera"
date: "5/31/2020"
output: html_document
---

```{r setup, include=FALSE}
library(readxl)
library(tidyverse)
library(plotly)
library(tidymodels)
library(doParallel)
library(scales)

conmebol_raw <- read_csv("conmebol.csv") %>%
  select(-X1, -rating) 

conmebol_raw %>%
  mutate(value = parse_number(value)) %>% View()
```


# Data Cleaning

```{r}

# Cleaning the value column deleting text and getting the right value of the player.
conmebol <- conmebol_raw %>%
  mutate(value = parse_number(value), 
         value = case_when(value < 99 ~ value * 1000000, 
                           TRUE ~ value * 1000)) %>%
  na_if(., 0) %>%
  select(-height, -name)
  

# Quick view and summary of the data at hand.
skimr::skim(conmebol)

```


## Data Split

```{r}

set.seed(415)

# Split data into training and testing data sets. 
conmebol_split <- initial_split(conmebol, prop = .8, strata = foot)
conmebol_train <- training(conmebol_split) 
conmebol_test <- testing(conmebol_split)

```


## Fitting Statistical Models - 

- Linear Regression Model
- Linear Regression Model with data Pre-processing. 
- Regularization Model - Lasso Regression
- Random Forest Model - XGBoost

### Linear Regression Model - (Using this model as baseline)

```{r}
set.seed(415)
linear_fit <- lm(value ~ . -id, data = conmebol_train)
summary(linear_fit)

```


Adjusted R-Squared - 0.69
Residual Standard Error -  $5,503,000

This simple linear model serves as a guideline to improve future statistical models.


### Linear Regression Model after Data Pre-Processing

**Data Pre-processing**

How far can we go with a simple linear model and working on data pre-processing. 

The following pre-processings steps have been performed.

- Log transformed the predictor variable *Value* of the player.

- Removed highly correlated features to prevent multicolinearity. 

- Narrowed *best_position* feature to values with a rate of ocurrance greater than 5%.

- Created dummy variable for all categorical variables.

- Impute the outcome *Value* using K-Nearest Neighbors.

- Centered and scaled all predictors.

- Removed possible variables that could contain a single value.

- Removed the *id* variable


```{r}

conmebol_rec <- recipe(value ~ ., data = conmebol_train) %>%
  update_role(id, new_role = 'id') %>%  
  step_log(all_outcomes()) %>%
  step_corr(all_numeric(), -all_outcomes()) %>%
  step_other(best_position, threshold = 0.05) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_knnimpute(value, neighbors = 5) %>%
  step_normalize(all_predictors(), -all_outcomes())  %>%
  prep()

conmebol_train_proc <- bake(conmebol_rec, conmebol_train) %>% select(-id)
conmebol_test_proc <- bake(conmebol_rec, conmebol_test) %>% select(-id)

  
```

**Linear Regression model with Pre-processed Data**


Results look promising with a simple linear regression after doing data pre-processing. However, results on the training data set suggest the model has severely overfited the training data. 

```{r}

linear_fit_proc <- lm(value ~., data = conmebol_train_proc)
summary(linear_fit_proc)

linear_fit_proc_pred <- predict(linear_fit_proc, conmebol_test_proc) %>%
  as.data.frame() %>%
  mutate(pred = exp(.)) %>%
  bind_cols(conmebol_test_proc) %>%
  mutate(truth = exp(value)) %>%
  select(pred, truth) 


# Root Mean Squared Error Function
RMSE = function(m, o){
  sqrt(mean((m - o)^2))
}


attach(linear_fit_proc_pred)
rmse_test <- scales::comma(RMSE(pred, truth))

```

Root Mean Squared Error on the Validation Set. 

`r rmse_test`


### Regularization Models - Lasso Regression


```{r}


X <- model.matrix(value ~ ., conmebol_train_proc)[, -1]

Y <- (conmebol_train_proc$value)



ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

plot(ridge, xvar = "lambda")


lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)


par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```


```{r}

# Ridge model
ridge_min <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Lasso model
lasso_min <- glmnet(
  x = X,
  y = Y,
  alpha = 1
)


par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```



**Tidymodels - Parsnip Package**


# Lasso Regression

Since this method is not working I had to try all the previous steps to catch what might be going wrong with the Lasso model using tidymodels. 


```{r}

lasso_spec <- linear_reg(penalty = 0.1, mixture = 1) %>%
  set_engine('glmnet')

lasso_fit <- workflow %>%
  add_model(lasso_spec) %>%
  fit(data = conmebol_train)

lasso_fit %>% 
  pull_workflow_fit() %>% 
  tidy()


```


# Tuning Lasso

# Not Working

```{r}
set.seed(415)
doParallel::registerDoParallel()


# Creates ten folds of the training data set.
tidy_kfolds <- vfold_cv(conmebol_train, v = 5)


lasso_tune <- linear_reg(
  penalty = tune(),
  mixture = 1
) %>%
  set_engine('glmnet')

lambda_grid <- grid_regular(penalty(), 
                            levels = 50)

lasso_grid <- tune_grid(
  workflow %>%
    add_model(lasso_tune),
  resamples = tidy_kfolds,
  grid = lambda_grid
)


```




```{r}
lasso_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```


# Non-parametric model - K-Nearest Neighbor

# Not Working

```{r}


knn_spec <- nearest_neighbor() %>%
set_engine('kknn') %>%
set_mode('regression')


knn_fit <- knn_spec %>%
  fit(value ~., data = bake(conmebol_prep, conmebol_train))

tru_pred <- predict(knn_fit, bake(conmebol_prep, conmebol_test)) %>%
  bind_cols(bake(conmebol_prep, conmebol_test)) %>%
  select(value, .pred) %>%
  rename(truth= value,
         prediction = .pred)



```

